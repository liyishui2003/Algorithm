2025/09/19 收到字节系-懂车帝面邀，考虑到准备时间有限，故在3.5天内针对实习、项目、针对性八股进行重点突击 + 刷30道hot lc 找回手感。
2025/10/11 收到字节-Tiktok音乐方向后台面邀，继续完成上次未竟的整理。

 

## 实习：

经历描述：期间独立主 R 小型需求，完成方案设计、技术评审、代码开发、自测联调、对接产运 QA、上线在内的全链路流程，有 DDD 分层架构与 CleanCode 意识。参与稳定性治理工作，完成日志治理解决频繁 gc/重复告警/有效日志不足问题；完成 i18n 多时区治理支持出海开国业务；完成 Device 服务拆分实现领域闭环与职责分离，负责Job 任务的灰度改造/Stage 环境测试/切流打点观察。有独立分析告警指标、排查问题并给出可靠结论的能力。
可能的问点：

#### 你对DDD的了解是？界限上下文怎么划分？
传统的MVC架构在开发的后期有天然劣势。比如一个订单服务，可能包含订单接口、评价接口、支付接口等，order表里存储了所有字段。维护代码时可能只想改下评价相关功能，却影响创单核心路径。根本原因是划分出来的模块高耦合，实现层面所有代码都写在一个serverimpl里。

传统的重构方式可以抽出单独的类和通用方法，但很难有业务层面的意义，只是为了代码好看。这导致新同学开发时难以理解，而DDD可以很好解决这个问题。

我对DDD的理解：本质上是对业务进行自然语义上的拆解，拆解成多个领域对象后将相关行为和数据注入，把业务逻辑和业务状态聚合在特定领域内，使其"充血"。

比如做抽奖平台时，抽奖平台是一整个领域。而用户划分为运营和消费者，这样能划分出M端和C端两个限界上下文。

接下来以C端为例，分析产品的需求。抽象活动有如下限制：
次数限制、时间限制、一个活动包含多个奖品、奖品本身有配置（库存、概率、被同一个用户获得的次数）、用户有新老和地域之分、活动本身有风控限制。

提取上述概念形成以下几个限界上下文：抽奖（核心子域，包含了用户和奖池，其中奖池对象的行为是为一组用户设置多个奖品并输出抽奖结果。奖品配置、概率、用户划分都在这个子域里维护），活动准入（维护次数和时间限制），库存（维护奖品核销，且本身具有通用性，可以独立出来），风控（这块显然是独立的）。

划分界限上下文我的理解是整个DDD设计里最难也最核心的，要根据语义去把能高度关联的需求在聚合一起，同时理清各个上下文之间的关系。比如这里抽奖要依赖库存、活动准入、风控，但库存和活动准入本身相对独立。

这些做完才是对代码层面的细化，如将领域行为封装到领域对象中，将资源管理行为封装到资源库中，将外部上下文的交互行为封装到防腐层中，承接新需求时就是对各个领域进行编排。



#### CleanCode意识在哪里体现？
这个有标准的规范，比如很出名的阿里巴巴Java开发指南，我自己的个人习惯是：每个方法尽量不要超过50行；变量名和方法名必须让读的人知道我要干什么；不写魔数；注释足够清晰；一个参数不会到处透传到底，会产生很多冗余，宁可多写几层做好PO/VO/DTO的转换；缩进和换行整个团队用同一套rules；重复的代码尽可能抽成通用方法；少写if-else嵌套来减少圈复杂度；承接需求时不做定制化，尽量做成通用接口。


### 怎么解决频繁gc的？
这个问题最开始是压测时发现gc问题频繁，查看堆对象有很多string、byte这类日志相关对象，初步考虑日志问题引起的gc。第二次压测时把日志打印取消，就没gc了，所以判定为日志打印引起。

解决时最开始老板希望我对过大的日志量做个抽样，比如每1%才打印一次。我做方案调研时发现日志抽样是个相对低频的技术方案，且抽样要考虑到分级别抽样，info级别可以不全大，error肯定要全打。随机也有随机的讲究，是对一个traceId取模打印呢，还是核心链路和非核心链路的随机数是不是也要不同。在实现层面，手写一个过滤工具类再手动加上显然是不现实的，稍微好一点的方式是切面编程aop，再加注解，但可能增加性能负担，此消彼长下效果如何有点存疑。

卡住时突然想到不一定要做抽样，本质上可能是个伪需求，既然日志打印很多，那就看看多在哪里，是否都有必要，抽样不如删掉。为什么不删掉？登机器一看发现几分钟能打500M，其中有一部分是debug日志打印线程情况，还有一部分是打印大JSON对象，但这些根本没法看，对研发排查问题没什么帮助。最后去掉并上线，gc就下来了。



### 讲一下gc原理
gc是对垃圾内存的自动回收，之前写cpp时要用malloc和free手动申请内存和释放，否则会造成内存泄漏。Java有一套完整的gc机制，程序员就不用手动管理内存。

判断一个内存是否还在用有一种朴素的思路：维护对该内存的计数引用，如果为0则废弃。但这样1会带来额外开销，2无法解决循环引用的问题。所以java里用的是可达性分析算法。

它的基本思想是从被称为GC Roots的根对象触发，遍历对象引用图，不被遍历到的就算可以被回收的。GC Roots有很多，比如常量或静态属性引用的对象，基本数据类型对应的class对象，系统类加载器等。这些对象肯定是活着的。

我学gc时有一个疑问：万一不可达的对象其实还有用，只是暂时不可达，真的可以安全回收吗？

后面想明白了只能通过引用访问对象，没有引用就无法访问，没有意义保留。至于不可达但还有用这个问题，场景很少，除非是用反射去强行访问，这个行为就是未定义的了。

至于gc怎么回收，朴素的思路是打标记-清除。由于清除会产生大量不连续碎片，所以又搞了复制算法，内存只用一半，清理时把还活着的内存统统挪到另一半，再一口气清掉刚用过的这一半。引起的另一个问题是内存变成一半了，且复制也有开销，对于一些存活率很高，需要被回收的数目少的对象，每次把一大个对象搬来搬去很麻烦，针对这个问题提出了标记-整理方法，让所有存活对象通过指针向同一方向移动，均摊下来会减少复制开销。

现代虚拟机采用分代收集算法来gc，这种算法是上面内容的结合，根据对象的生命周期不同将内存划分为几块，每块采用最适当的收集算法。大批对象死去、少量对象存活的（新生代），使用复制算法；对象存活率高、没有额外空间进行分配担保的（老年代），采用标记-清理算法或者标记-整理算法。


### gc还可能是什么情况导致？



### 日志打印不当可能造成哪些问题？


### Job任务的灰度放量具体怎么放，灰度是怎么实现的，切流又是怎么实现的？
对http流量的请求头里塞入版本；根据地域进行灰度；转发的时候转发到对应的灰度实例或者原有的实例。
具体怎么灰度跟中间件有关系，网关请求类就按配置路由规则去灰度；服务发现类按实例分组；异步消息队列类的按灰度消费组灰度。

### 服务发现、注册类中间件原理？配置中心原理？
注册中心就像服务的通讯录。服务上线时会向注册中心注册自己的元信息(IP、端口、服务名)，通过SDK或者http向服务中心发起注册请求。注册中心收到后会把信息写到本地注册表并持久化。接下来provide每隔一段时间主动发送心跳包，部分注册中心也会主动进行健康检查。服务下线时正常会调用相关端口，不正常的话探测失败到一定次数就把它下线。
CAP理论中C是consistency一致性，A是avaliable可用性，P是partition tolerance分区容忍性，对于分布式系统来说P是必须的，那就只能在C和A之间取舍了。
CP的经典例子是zookeeper，它有个ZAB协议来确保一致性。有个leader专门负责写请求，follower处理读请求，转发写请求，同步数据。虽然保证了强一致性，但lead挂了且在选举期间，注册中心就不可用了。
AP的经典例子是nacos，优先保证可用性。注册中心挂了客户端也有本地缓存，最后能一致就行。nacos的AP是通过服务唯一对应主节点，请求会被路由到该节点。写入内存并落库后就会返回成功，再异步同步给其它节点。

#### 介绍一下zookeeper和ZAB协议
zookeeper是一个分布式协调服务系统，可以用来统一配置，服务注册，集群管理。
写时lead会发起请求，过半数节点同意后会提交该事务，所有节点都执行。从任意节点读取到的都是最终数据。
宕机时的选举机制是互相投票，节点收到她人投票后如果对方的zxid或者myid更大，就改投对方。不断交换选票直到出现lead。期间注册中心无法使用。
ZAB协议需要奇数个节点是因为同样容错能力下，节点数更少，协商更快。最重要的是确保多数派只会有一个，能防止脑裂，或者怎样都选不出lead的情况。


#### 负载均衡算法
nginx用的是经典的roundrobin，指针递增，对总数取模。支持加权和健康检查，比如一定时间内失败次数超过一定阈值标记为不可用，暂时移出轮询队列，后面会尝试再将请求分配给实例。不绑定会话session，同一客户端请求会被分配到多个实例。
dubbo的负载均衡会强一点，因为是阿里的rpc框架，对负载均衡要求高点。它的轮询是客户端负载均衡，消费者本地通过注册中心同步提供者列表，并独立轮询。支持权重动态配置（消费者本地可直接同步）和平滑加权轮询（基础权重和当前权重的设置有助于小权重实例被选中，避免请求全部集中打到大权重实例）。健康检查是基于主动心跳和熔断机制。

### 既然提到了dubbo，讲一下rpc框架通常都是怎么设计的，解决什么问题，怎么实现跨语言通信？
调用远程服务时能像调用本地一样。过程：
rpc是远程调用的缩写，本质上是：
1.客户端调用本地的代理函数stub
2.stub负责把参数序列化成可传输的字节流
3.stub调用本地socket api(如send)触发系统调用进入内核态
4.内核将数据通过网络协议栈(TCP/UDP)发出去，服务端内核收到后放到内核缓冲区，唤醒监听的服务端进程，把数据拷贝到用户态
5.服务端进行反序列化，调用真正的业务函数处理请求，并把结果返回给服务端stub
6.stub将结果序列化后，通过内核态发送给客户端
7.客户端内核接收数据，拷贝到用户态，stub对结果反序列化，返回给调用方
其中可以展开讲的有很多，比如说：
1.stub代理可以通过动态代理实现，在java里用的是jdk动态代理生成代理类（那个很出名的设计模式代理模式），重写invoke方法。运行时会拦截请求执行代理类的逻辑（参数序列化、网络发送）
2.序列化和反序列化模块，最常见的JSON,protocolbuffer(用二进制编码，自带编译器，速度快，跨语言)。
这里可以拓展下：
protobuf的快来自于变长编码，避免反射（阿里的jackson和json都有运行时反射，慢很多）直接操作字节，字段重编号，默认值，总之怎么抠门怎么来
3.网络协议层，grpc用的是http2.0（自带单TCP多路复用，消息头压缩这些特性，速度更快），dubbo有自己的dubbo协议
4.服务端的监听是epoll监听吗，是IO多路复用吗。
是的，就是c++实现服务器都会遇到的epoll。因为rpc面对海量连接，必须复用。
5.比较一下各个rpc框架
grpc特点是protobuf和跨语言，dubbo主打一个java为主，有强大的微服务治理，thrift则是可选项非常多，语言协议序列化都可以自己搓，文档也很少，所以适合大公司自己魔改。

### 动态代理原理？为什么说反射会慢很多，反射原理？

### 告警后的解决思路？主要看哪些指标？排查过什么问题觉得比较有成就感？
比如说监控发现mysql cpu内存占用较高，连接池较满，有哪些原因导致的，可能有哪些排查方向？

 
项目：
### 讲一下事件驱动模式？
I/O多路复用
概念：指在单线程或单进程环境下同时监控多个 I/O 事件的技术。
原理：使用一个系统调用(select、poll、epoll等)同时监听多个IO源的状态变化。
程序将需要监控的IO源(通常是文件描述符)注册到这个系统调用中，然后阻塞等待。
当其中任何一个或多个IO源发生变化(如可读、可写、出错等)时，系统调用会返回，并告知程序哪些IO源已经准备好进行相应的操作。
程序根据返回的信息，对准备好的IO源进行读写操作。
也就是说通过系统调用，起到一个监控的作用，同时监控很多IO。

### select/epoll了解过吗？
select:
#include <sys/select.h>
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
参数依次是：
nfds：需要监控的最大文件描述符值加1；readfds：指向一个fd集合，监控读事件；writefds：指向一个fd集合，监控写事件；exceptfds：指向一个fd集合，用于监控这些文件描述符的异常事件；timeout：指定 select 函数的超时时间。
工作流程是：
使用时初始fd_set集合，将需要监控的文件描述符加入，调用select函数。
原理是：
监控时遍历所有监控的文件描述符，很慢；能监控的fd也有限，一般是1024个；每次调用都要传入这么多参数，有的fd可能被反复传了很多次。

epoll:
工作流程是：
调用 epoll_create 创建一个 epoll 实例；使用 epoll_ctl 函数向 epoll 实例中增删改需要监控的文件描述符和事件;调用 epoll_wait 函数进行阻塞等待。当有事件发生或超时后，epoll_wait 函数返回，检查 events 数组来确定哪些文件描述符有事件发生。
原理是：
红黑树+链表。红黑树实现快速增删改，链表存储有变动的事件调用 epoll_wait 函数时，内核会将链表复制到用户空间，通知用户哪些fd准备好了。
红黑树的增删改是O(logN)的，返回的链表也只包含了需要处理的事件，没有冗余信息。

边缘触发模式和水平触发模式
epoll有两种工作模式，边缘触发(Edge Triggered，ET)和水平触发(Level Triggered，LT)。
ET：只有fd状态变化时(如从无数据变为有数据，从不可写变为可写)，才会触发事件通知。
用户必须一次性处理完所有的数据，否则剩余的数据将不会再次触发事件。
ET 模式是一种高速模式，适合处理大量的并发连接，但对编程要求较高。需要搭配非阻塞套接字使用。
如果用阻塞套接字，读到中途没数据读时，read函数会阻塞当前线程，直到有新数据到达。
但新数据到达前不会再次触发事件，就可能导致部分数据没法读，就这么丢了。

LT：只要文件描述符的状态满足事件条件(如有数据可读，可写)，就会一直触发事件通知。即使一次没有处理完所有的数据，下次 epoll_wait 仍然会再次通知。

### 一个请求打过来，在每个组件之间流转的流程？
采用的模式是1个主reactor+N个从reactor。
epollloop：每个reactor一个
channel：IO事件封装器，每个socket对应一个channel，存储该套接字感兴趣的事件和
事件回调函数。
acceptor：专门处理新连接的组件
connection：封装已建立的客户端连接
过程：
#### 1主reactor
初始化：主epollloop初始化，acceptor创建监听socket并监听对应端口，注册对应的acceptchannel到主epollloop里。线程池也初始化，数量通常等于CPU核心数
客户端发起连接：
客户端通过connect发起TCP连接后，服务器端监听到fd变成可读，epollloop的系统调用epoll_wait找出就绪事件列表，其中包含该可读事件。接着找到该fd对应的acceptchannel并调用其handlevent，触发acceptor的回调。
acceptor接收连接并分发：
acceptor调用accept() 接收新连接，得到客户端连接的 Socket（connfd）。并将其配置为非阻塞模式，通过负载均衡策略选择一个从epollloop，将connfd交给该从reactor。
#### 2从reactor
从epollloop所在线程接收到新的connfd后，创建connection对象封装connfd以及连接状态，并创建channel绑定回调函数（handleread（）当有可读数据时触发，负责读取，handlewrite（）当有可写时触发，负责发送响应数据），并将该channel注册到epollloop。
至此TCP连接完全建立。
#### 3处理客户端请求
客户端通过 send() 发送请求数据，服务器端 connfd 变为 “可读”，EpollLoop 的 epoll_wait返回就绪事件，找到connfd对应connChannel，调用其handleEvent方法，触发 Connection的回调handleRead，调用read()读取数据。
接下来就算解析http头或者反序列化rpc参数等。生成响应后通过connection的send并发送。
（这里发送有个实际的问题要考虑，调用write进行发送时，如果内核缓冲区满了，要在connChannel中注册“可写事件EPOLLOUT”，等内核缓冲区空闲后继续发送）
#### 4连接关闭
客户端调用 close()，服务器端 connfd 触发 “可读事件”，但 read() 返回 0（表示 EOF）。Connection::handleRead() 检测到 EOF 后，触发 handleClose() 回调。

### 上述代码里智能指针优化什么了？
1.避免野指针
Connection 对象代表一个客户端连接，若客户端断开连接时，Connection 被销毁，但此时可能仍有一个 “数据可读” 的事件回调正在另一个线程中执行。
若使用原始指针，此时回调访问已释放的Connection内存，会导致野指针崩溃。
智能指针的优化：muduo 中 Connection 等核心对象通过 std::shared_ptr 管理，所有持有对象引用的地方（如事件回调、业务逻辑）均使用 shared_ptr。
当 Connection 需要销毁时（如连接断开），仅减少 shared_ptr 的引用计数，若此时仍有回调持有引用（计数 > 0），对象不会被立即释放，直到最后一个引用消失。
回调函数中通过 shared_ptr 访问对象，确保操作期间对象一定存活，彻底避免野指针。
同时shareptr的特性是线程安全(计数器的加减是原子操作)，也能在线程间安全传递。

2.避免内存泄漏
Connection 的 shared_ptr 会被 Channel、事件回调、业务逻辑等持有，当所有持有方都不再需要该对象（如连接断开后，Channel 被移除、回调执行完毕），引用计数归 0，对象自动析构（调用 ~Connection()，内部关闭 connfd 并清理资源）。


## redis相关：
### redis整个底层实现了解吗？解决什么问题的，原理？
关注数据结构比较多。印象深刻的是跳表。这在kafka里也用过，是一个很精妙的设计。朴素的链表无法实现快速查找只能O(N)，和数据库一样经典加速方式是索引。加了索引后只要跟索引比较就好。然后索引之上再加索引，跳表就是加了很多级索引的链表。这是定性的认知，定量上每一层索引节点个数大概是下层的二分一，所以层数是log2N。删除、查找和新增的复杂度是log。zset的底层就是用跳表+哈希表实现的。
另外一个印象比较深的数据结构是压缩链表ZipList，适用于数据量小的情况。原理是避免了指针开销，内存紧凑，但维护起来复杂。

### 常用的集群结构有哪些？
主从复制，哨兵模式，分布式分片集群。
主从：1 个主节点+多个从节点，主节点负责读写操作，从节点仅负责读操作。
分担主节点读压力，弊端是主节点故障后需要手动切换，无法存储海量数据。
哨兵：主从复制基础上增加哨兵节点（Sentinel），通常部署3个以上哨兵，
不存储数据，仅负责监控、故障转移。当主节点宕机（主观下线、客观下线），哨兵通过投票机制从从节点中选举新主节点（超半数投票，如果竞选失败等待一段时间重新竞选），并更新其他从节点的复制目标，但数据量仍然受限制。
分布式分片：放在下面这个问题讲。

### redis cluster如何找到对应节点
这种集群模式强大的地方在于水平扩展，容错率高。
水平扩展原理是每个节点给新增节点分配一些槽，该过程会遍历槽内所有的键然后原子性地搬过去。对于客户端来说，可以从任何一个节点获取槽的分配信息，计算出所需数据对应的槽后，和对应节点通信即可。
单个节点故障了不会影响其它节点，且从节点很快会自动变成主节点顶替上来。
找对应节点的方式：
'''
CRC16(key) % 16384
'''
其中CRC16将任意长度的键（字符串） 转换为一个固定长度的 16 位整数，计算快，算法轻量。% 16384则是计算对应哈希槽，16384是设计过的数字，既可以支持3个节点的小规模集群，也可以支持100个节点的大规模集群。redis集群通过gossip协议同步槽-节点这一映射关系，槽如果太多会增加同步成本，16384的成本只要2kb，符合预期。



### 如何实现分布式锁？
分布式锁要满足的特性：
互斥（同一时刻只能有一个客户端持有）、避免死锁、可用（持有它的客户端崩溃，也能释放）、安全（只能被持有它的客户端释放）
redis实现分布式锁的原理是redis的原子操作。
'''
SET lock_key unique_value NX PX 10000
'''
需通过 Lua 脚本保证原子性，实际生产中有成熟的redisson框架。
redisson的核心特性是有个看门狗机制，能自动延长有效期。
当时好奇了下看门狗机制是怎么自动延长的，因为业务的执行时间是没法预判的。实际上它的逻辑是用后台进程去响应，通过redis的exist命令检查，只要客户端还持有锁，就不断通过lua脚本原子性地续期，直到业务主动释放或者客户端崩溃（此时看门狗也会停止，锁会在最后一次续期后释放）。
但没能解决的问题是如果redis单点故障，会导致锁管理失效。
解决方案是部署哨兵集群或者redlock算法（本质上也是投票算法，在多个独立 Redis 实例上获取锁，需多数节点成功）。


### redis里的expire怎么实现，你来写怎么写？



消息队列相关：
partion和消费者的数量关系；
如何保证不丢消息；
下游怎么保证幂等；
rabiitMQ在这个项目里怎么用的？
有没有对自己的系统做过测试？
 
针对性八股：
408复习（见：）
多线程相关：
线程同步和线程安全一个问题吗？
三个线程循环打印abc，abc，abc？
三线程轮流打印1-1000；
synchronise怎么做到一个操作，怎么保证线程同时赋给自己，又赋给主内存？
MySQL:
#### 索引的数据结构为什么不用其它的要用B+树
叶子节点通过双向链表连接，便于范围查询，这点上比哈希表强
非叶子节点仅存储索引键，不存储数据，数据只存在于叶子节点，降低树高，减少IO操作次数，3-4层的B+树就可以支撑千万级数据；
本身是平衡树这点也避免了退化成链表，树高也比同样是平衡树的红黑树少。但是分裂和合并维护起来很麻烦。尝试手写过b+树未竟，合并的适合要往左往右借节点，解决不了的还要往上报，逻辑和细节都很长。。


索引机制；
索引失效；比如说我有一个联合索引abc，查询是select from 表 where b> And a= 会用到索引吗，那a = 1 b >2 c=3呢；
### 慢查询；
没索引
索引没用上（！= / not in /not exist/不满足最左前缀原则）
索引冗余，粒度太粗
sql写得太差了（select* 未限制返回行数 join表连接太多）
数据量本身太大


### 表空洞怎么处理；
频繁的删除和update操作后会使得原位置不再使用，产生空洞。
可以用自带的OPTIMIZE TABLE，原理是重建表和索引，整理空闲空间使得数据连续存储。但这样会锁表，只能在业务低峰期使用。MySQL5.6+的版本支持在线无锁优化。

Binlog怎么看；
mvcc隔离等级，重复读，读提交，快照读；
### delete是否加行锁，数据怎么安全地删掉


page是什么，页是什么；
Java相关：
spring；
ioc；
aop；
单例模式实现；
