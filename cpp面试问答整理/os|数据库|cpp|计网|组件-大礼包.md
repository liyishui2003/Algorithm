# CPP
## STL
#### 1.迭代器失效问题
- 树形迭代器(map/set/multimap/multiset)的插入和修改都不影响其它迭代器，删除后被删除元素的迭代器失效
- 链表形迭代器(list)同上
- 数组形迭代器(vector/deque)，被操作元素及其后边的元素迭代器都失效(如果capcity发生改变，则全部失效，这里特指vector的resize())

#### 2.vector
- 分配是在堆还是在栈？在堆。因为要求下标连续，栈内没法保证连续；其次vector需要扩展，栈内也没法扩展。
- resize和reserve有什么区别？resize用来改变实际存储的元素数量(size)，如果比原来小，会删除末尾元素，如果比原来大，会添加默认值。
而reserve改变的则是预分配内存大小(capcity)，如果比原来小，不会做任何事情，如果比原来大，会重新分配内存。
- 如何改变size大小？resize(),push_back(),pop_back(),clear(),insert(),erase()
- 如何改变capcity大小？reserve(),shrink_to_fit()

#### 3.map
- map是线程安全的吗？不是，这只是一种数据结构，设计的时候没有考虑多线程并发。要保证线程安全得加锁
- map和unordered_map的效率比较？时间上map是O(logN)，unordered_map是O(1)，最坏情况下退化成O(N)。要考虑缓存和空间的话，map的缓存连续性差，因为不连续。
- unordered_map的底层？哈希表。
- map的底层(即红黑树原理)？节点分为红色黑色，插入时大力分类讨论，通过左旋或右旋来保证树的高度维持在log。
- 讲一下开链法？线性探测/二次探测，和链地址法那样每个下标带一个链表不同，开链法在遇到哈希冲突时会探测下一个可用的桶来解决冲突。
  
#### 4.一致性哈希算法和普通哈希算法有什么区别？
普通哈希在N或者模数改变时，几乎大部分数据都要重新映射，不利于实践中服务器扩容或者处理服务器崩溃。
而一致性哈希则采用固定模数2^32-1，将哈希空间映射成一个很大的环，服务器根据编号映射到环上。查找数据时查找顺时针离自己最近的服务器，扩容时只要添加新的节点，服务器崩溃时请求也可以走向下一个服务器。
因此一致性哈希常用于分布式缓存如Redis。

#### 5.数组和链表的区别？太简单，略。

#### 6.栈和队列的区别？太简单，略。



## 智能指针
#### 0.智能指针有哪几种？
share/weak/unique/auto

#### 1.shareptr是不是线程安全的？
否，shareptr只通过原子操作保证计数是线程安全的，但多线程同时访问shareptr所管理的对象时依然会有并发问题，需要加锁。

#### 2.把uniqueptr移动到shareptr会发生什么？
uniqueptr的为空，不再拥有资源，shareptr创建一个新的控制块来管理资源。底层伪代码是：
```cpp
template<typename T>
std::shared_ptr<T>::shared_ptr(std::unique_ptr<T>&& uniquePtr) {
    if (uniquePtr) {
        // 接管资源
        this->ptr = uniquePtr.release();
        // 创建控制块
        this->controlBlock = new ControlBlock();
        this->controlBlock->refCount = 1;
    } else {
        this->ptr = nullptr;
        this->controlBlock = nullptr;
    }
}
```

#### 3.weak_ptr是为了解决？
为了解决shareptr的循环引用，比如类A声明了类B的shareptr，类B声明了类A的shareptr，这俩指针就永远不会被释放。
而weakptr的思想就是只观测而不增加引用计数，好比房产中介知道所有房地产的信息(通过lock()方法能安全访问shareptr)，但是不拥有房产。
因此weakptr也很适合用在观察者模式。

但要注意的是，weakptr不是真的不引用，它有一个弱引用计数。弱引用计数是为了计算有多少个weakptr在观察shareptr，当shareptr的引用计数归0时，weakptr并不会被释放。
只有当弱引用计数也归0时，控制块和weakptr才真的会被释放。这么设计是为了确保weakptr可以正确检测shareptr是否被正确释放。

#### 4.share_ptr的底层实现？回收原理？
```cpp
#include<iostream>

template <typename T>
class shared_ptr{

private:
	T* ptr;
	size_t* cnt;
	/*
	将引用计数减1，如果减后变为0
	说明没有其他实例引用该对象，于是释放占用的内存。
	*/
	void realse(){
		if(cnt && --(*cnt) == 0){
			delete ptr;
			delete cnt;
		}
	}
	
public:
	// 初始化
	explicit shared_ptr(T* _ptr = nullptr):ptr(_ptr),
		cnt(_ptr ? new size_t(1) : nullptr){
		
	}
	// 拷贝操作
	// 传入other，令this等于other并修改引用计数
	shared_ptr(const shared_ptr& other):ptr(other.ptr),cnt(other.cnt){
		if(cnt){
			++(*cnt);
		}
	}
	//赋值操作
	//shared_ptr& 表示返回的是一个引用，除了减少开销外
	//还可以支持a = b = c这样的操作
	shared_ptr& operator=(const shared_ptr& other){
		/*
		在赋值运算符重载函数中，通常是比较对象的地址是否相同
		常见的做法是将 other 的地址与 this 进行比较，即 this != &other
		如果写的是 *this != other，要重载 == 运算符
		*/
		if( this != &other){
			realse();
			ptr = other.ptr;
			cnt = other.cnt;
			if(cnt){
				++(*cnt);
			}
		}
		//这里this前加*号是this是指向当前实例的指针
		//解引用后才能返回实例(引用)
		return *this;
	}
	//析构函数
	~shared_ptr(){
		realse();
	}
	// 有时可能需要对指针进行算数运算，或其它需求
	// 总之get()用来支持返回原指针
	T* get() const {
		return ptr;
	}
	// 常规的解引用
	T* operator->() const {
		return *ptr;
	}
	T& operator*() const {
		return *ptr;
	}
	size_t getCnt() const {
		return cnt ? *cnt : 0;
	}
};
class Myclass{
public:
	Myclass(){
		
	}
	void doSomething(){
		
	}
};
int main(){
	shared_ptr<Myclass> ptrA(new Myclass());
	std::cout<< ptrA.getCnt() << '\n';
	shared_ptr<Myclass> ptrB;
	ptrB = ptrA;
	std::cout<< ptrA.getCnt() << '\n';
	std::cout<< ptrB.getCnt() << '\n';
}
```

以上是手写版，真正底层是维护一个
控制块(Control Block)，含引用计数/弱引用计数/删除器/指向对象的指针，回收是基于引用计数，引用计数为0时删除指向对象，但不删除控制块。建议使用shareptr时用make_shared，它将对象和控制块的内存分配合并为1次，但也导致了这俩内存绑定在一起，
弱引用计数为0时才会一起释放。

## 语法
#### 1.struct和class区别？底层是如何实现的？
最大的区别是class默认成员权限和继承方式是private的，而struct则是public的，其它几乎完全相同。
但struct是C语言的遗产，设计主要用于POD类型，没有复杂的封装和析构/构造函数等。

#### 2.虚函数的实现原理
虚函数表+虚函数指针。

虚函数指针存放在对象所在内存空间最开始的部分，指向虚函数表。当然如果继承自多个对象，在内存空间的布局上会按顺序依次排开：
基类A的虚函数指针，基类A的成员1，基类A的成员2...基类B的虚函数指针，基类B的成员1，基类B的成员2。

虚函数表则存储了该类所有虚函数的地址，如果派生类重写了虚函数，虚函数表会更新为派生类的函数地址。
同理，如果继承自多个对象，也会有多个虚函数表。

虚函数被诟病为效率低，但事实上真的计算起时间效率(不过多了几次间接寻址)和计算空间效率(多维护了几个指针)的话，如果继承关系不复杂，其实差不多。
最主要的开销是只能在动态运行时绑定，因此无法在编译阶段优化，并且多重继承时要维护很多的虚函数表，哪怕用不上，比如GUI库就是这种情况。

#### 3.构造/析构函数允许是虚函数吗？
构造函数不可以，最主要的原因是虚函数使用了虚函数表（vtable），在调用构造函数时，对象还没有完全创建和初始化，所以虚函数表可能尚未设置。
在构造函数中使用虚函数表会导致未定义的行为，也会被编译器禁止。
此外，虚函数是为了实现覆盖基类的成员函数，但每个类都有自己的构造函数，构造对象时编译器知道正在构造的是哪个类，因此会直接调用该类的构造函数。
不需要通过虚函数机制动态绑定。没必要用构造函数来实现多态。

但基类的析构函数一定要是虚函数，否则删除基类指针时，只会调用基类的析构函数，不会调用派生类的虚构函数，导致这部分资源无法被释放。

#### 4.构造和析构函数里throw会有什么问题？
构造函数里throw没问题，还可以帮助确认资源是否被正确初始化。
析构函数里throw有大问题，因为遇到异常时C++的处理机制是会栈展开，往上找能匹配的catch块，如果此时析构函数抛出异常，遇到两个异常会直接terminate，会导致资源无法被正确释放。


#### 5.如果有一个类在析构的时候可能throw，怎么设计能保证正确析构并释放内存？
最好最好还是不要在析构时throw，但可以暴露出接口，让用户显式调用，比如将可能抛出异常的代码从析构函数中移出，放到一个普通的成员函数中(如 close() 或 release())中。
或者用RAII模式，写一个资源管理类，将资源管理与对象本身分离，主对象的析构函数可以抛出异常，但不会影响资源释放。


#### 6.C++11/14新特性，你常用的有哪些？
auto/智能指针(配套的make_unique)/移动语义/unordered_map/optional

#### 7.讲一下左值和右值/移动语义/万能引用和完美转发？
左值是可以出现在等式左边和右边的值，有名字有内存地址的值，而右值是不是左值的值。
更具体的，只能出现在等式右边，短暂地存在，没有名字也没有内存地址的值，比如int x = 20中的20，比如函数的返回值。

右值的出现是为了支持移动语义/万能引用/完美转发。没错，这几点都可以串在一起讲。
cpp primer里说到，原本的拷贝构造函数都会先复制出一个temp对象，再将temp对象复制到需要的内存卡中，再将临时对象删除。
考虑到临时对象被删除了，如果能直接转让这个临时对象的所有权给要赋值的对象，不是更好吗？而这种不真正移动，只修改记录的方法就是移动语义。
要实现移动语义，需要让编译器知道什么时候需要复制，什么时候不需要。因此可以定义两个构造函数，一个是常规复制构造函数，使用const左值引用作为参数；
另一个是移动构造函数，使用右值引用作为参数。

万能引用指的是T&&,原理是引用折叠规则(&遇到左值返回左值，遇到右值返回左值，&&遇到什么返回什么)，所以可以保留参数的类型。
通常和forward结合使用，forward的原理是写了两个都声明为forward的函数，一个接受左值，一个接受右值，返回类型都为T&&:
```cpp
template<typename T>
T&& forward(typename std::remove_reference<T>::type& arg) noexcept {
    return static_cast<T&&>(arg);
}

template<typename T>
T&& forward(typename std::remove_reference<T>::type&& arg) noexcept {
    return static_cast<T&&>(arg);
}
```
有了万能引用后，就可以实现完美转发。有如下应用：
- 工厂函数用于创建对象，并可能需要将参数传递给对象的构造函数。完美转发可以确保参数以原始的值类别传递。
- 实现容器的插入、删除等操作时，可能需要将参数以原始的值类别传递给容器内部的函数。比如a.push_back(42)
- 多线程任务传递时需要将任务和参数传递给线程池，完美转发可以确保参数的值类别不变。


#### 8.讲一下纯虚函数和抽象类，虚函数和纯虚函数的的区别？
纯虚函数是一种在基类中声明但没有实现的虚函数。它的作用是定义了一种接口，这个接口需要由派生类来实现。（PS: C++ 中没有接口，纯虚函数可以提供类似的功能）。包含纯虚函数的类称为抽象类（Abstract Class），抽象类仅仅提供了一些接口，但是没有实现具体的功能。作用就是制定各种接口，通过派生类来实现不同的功能，从而实现代码的复用和可扩展性。

两者区别为：
- 虚函数有默认实现，纯虚没有默认实现，用=0来声明。
- 虚函数基类可以实例化，纯虚不行
- 派生类可选是否重写虚函数，纯虚则必须要实现

#### 9.讲讲拷贝构造函数和operator=()
拷贝构造函数用于初始化新对象，赋值运算符则用来赋值给已存在的对象。
赋值运算符需要自赋值检查，两者都需要特殊处理深拷贝与浅拷贝。

#### 10.memcpy和memmove区别
最大的区别在于处理内存重叠时行为不同，memcpy不处理内存重叠，因而性能更强。
memmove则能正确处理内存重叠,如果源区域和目标内存区域重叠，memmove会确保拷贝后的数据是正确的。


#### 11.内存屏障了解吗？
了解。多线程或多核环境中，编译器和处理器可能会对内存操作进行重排序以提高性能。然而，这种重排序可能出问题。
比如
```cpp
int x = 0;
bool ready = false;

// 线程 1
void thread1() {
    x = 42;         // 写操作
    ready = true;   // 写操作
}

// 线程 2
void thread2() {
    if (ready) {    // 读操作
        assert(x == 42);  // 读操作
    }
}
```
没有内存屏障，编译器和处理器可能会对 thread1 中的两个写操作进行重排序，导致 thread2 看到 ready == true 但 x != 42 的情况。
内存屏障主要分为写屏障/读屏障/全屏障，可以用硬件或者软件来实现，C++11中则是std::atomic_thread_fence。

#### 12.值传参和引用传参的区别？效率有区别吗？
值传参是pass by value，有拷贝开销，不会影响实参；引用传参传引用，没有拷贝开销，但会影响实参。
效率方面因为没有拷贝，引用传参的效率远高于值传参。

#### 13.void* 指针能否做算数运算，为什么？有什么作用？
不能，算数运算的本质是移动类型对应的size，void连类型都没有，怎么移动。
void* 可以指向任意类型的数据，常用在泛型编程，比如C标准库中qsort和bsearch就是用void* 指针。
动态内存管理(malloc)时用的也是void*，因为不知道用户将会分配成什么类型的内存。同理，接口设计时如果
参数不定类型，也可以传void* 指针进去。

#### 14.const可以修饰全局函数吗？const全局变量在.data段还是.bss段？const局部变量存在哪？有没有办法修改const修饰的变量？const T &可以接受右值吗？
- const不能修饰全局函数，const 关键字只能用于修饰类的成员函数，表示该函数不会修改类的成员变量。全局函数没有类的上下文，因此不能用 const 修饰。
- 全局变量存储位置取决于是否被初始化。已初始化的const全局变量存储在.data段(只读数据段)，未初始化的全局变量存储在.bss段(未初始化数据段)。
- const局部变量存储在栈中，只是值不能被修改。
- 可以用mutable。
- const T &可以接受右值，因为const承诺不会修改对象，还可以延长其生命周期。而非 const 引用（如 T&）不能绑定到右值，因为右值是临时对象，非 const 引用可能会修改它，导致未定义行为。

#### 15.常成员函数的作用？
- 常成员函数的主要作用是保证该函数不会修改类的成员变量，常用于只读函数如get()或者打印对象状态
- 可以支持const对象调用

#### 16.空类大小，继承空基类的子类大小，继承空虚基类的子类大小？
- 空类大小为1字节
- 如果子类没有新增成员变量或虚函数，编译器可能会进行空基类优化（Empty Base Optimization, EBO）。在 EBO 优化下，子类的大小为 1 字节。如果没有 EBO 优化，子类的大小可能为 1 字节（基类） + 1 字节（子类） = 2 字节。
- 8字节，因为要存虚表指针。当然新增了成员变量或者虚函数，子类大小也会相应增加。

#### 17.new和malloc区别？C++的内存分布模型？
前者是c++运算符，后者是库函数。
- 分配内存位置：new在自由存储区，并且会调用对象的构造函数来初始化对象；malloc在堆上，只分配内存。
- 返回值：new为完整类型指针，malloc为void*
- 大小：new会自动计算，malloc必须自己指定
- 内存扩充：new没法直观处理，realloc可以简单完成
- 互相调用：new可以调用malloc，也可以不调用，毕竟new有好多种，可以连内存都不分配。但显然malloc不可以调用new。
  

#### 18.讲一下assert和static_assert
assert是断言，如果不满足条件就会中止，常用于开发环境。
static_assert是编译时如果不满足会直接中止，用于检查编译时的常量表达式，例如模板参数、类型大小等。
assert不会在编译时中止，用于检查运行时逻辑错误。

#### 19.不借助库函数如何判断大小端？什么时候需要注意大小端？
```cpp
bool isLittleEndian() {
    int num = 1;  // 0x00000001
    char* ptr = reinterpret_cast<char*>(&num);//reinterpret_cast是进行低级别的强制转换
    return *ptr == 1;  // 在这里其实是未定义行为，相当于输出第一个字节
    //而如果第一个字节是 1，则是小端
}
```
原理是利用char和int占用的字节差。

#### 20.多态实现和原理？
多态分为动态多态（基于虚函数的动态绑定）和静态多态（模板/函数重载），虚函数上面见过了，提一下后者。
- 模板：相当于给编译器提供了一个蓝图，编译器会拿着这个模板去生成对应的代码
```cpp
template<typename T>
T max(T a, T b) {
    return (a > b) ? a : b;
}
int main() {
    std::cout << max(10, 20) << std::endl;       // 输出 20
    std::cout << max(3.14, 2.71) << std::endl;   // 输出 3.14
    return 0;
}
```
- 函数重载：函数重载是指同名，但参数列表(顺序/类型/参数类型)不同。
以下就是一个复杂的函数重载。
```cpp
template<typename T>
T&& forward(typename std::remove_reference<T>::type& arg) noexcept {
    return static_cast<T&&>(arg);
}

template<typename T>
T&& forward(typename std::remove_reference<T>::type&& arg) noexcept {
    return static_cast<T&&>(arg);
}
```

#### 21.空类会自动生成什么函数？
默认构造函数(只分配内存)/默认析构函数(什么也不做)/默认拷贝构造函数/默认拷贝赋值运算符/
默认移动构造函数/默认移动赋值运算符/默认的new delete &运算符。
这是因为哪怕类是空的，也要保证它能被正确构造/赋值/移动/销毁等。

#### 22.介绍一下移动构造？
移动构造上面说过了，本质上是原本的拷贝构造太慢，所以引入了右值引用实现移动构造。
原理是通过右值引用绑定到临时对象或即将销毁的对象，然后“窃取”其资源。具体步骤如下：

窃取资源：将新对象的指针指向原对象的资源。
置空原对象：将原对象的指针置空，避免资源被重复释放。
```cpp
//例子一：
MyClass(MyClass&& other) noexcept {
        data = other.data;  // 窃取资源
        other.data = nullptr;  // 将原对象的资源置空
}
//例子二：
```


#### 23.栈区和堆区区别？在栈上新建对象和在堆上新建对象区别？
以下讨论的是程序内存里的栈区和堆区
- 内存管理：栈由编译器管理，堆区由程序员手动管理
- 生命周期：随作用域结束释放；需要显式释放
- 分配：移动栈指针，快速；查找合适的内存块，较慢
- 大小：较小，以MB为单位；较大，受系统内存限制
- 适用：局部变量，函数调用栈；动态分配的大型对象，长期存在的对象。

新建对象的区别：
```cpp
class MyClass {
public:
    MyClass() { std::cout << "Constructed\n"; }
    ~MyClass() { std::cout << "Destructed\n"; }
};

int main() {
    MyClass* obj = new MyClass;  // 在堆上新建对象
    delete obj;                 // 显式释放对象
    return 0;
}

//====我是分割线====
class MyClass {
public:
    MyClass() { std::cout << "Constructed\n"; }
    ~MyClass() { std::cout << "Destructed\n"; }
};

int main() {
    MyClass obj;  // 在栈上新建对象
    return 0;     // obj 自动销毁
}
```

内存空间布局为：


+---------------------+
| 栈区（高地址向下增长）|  
+---------------------+
|      自由映射区      |
+---------------------+
| 堆区（低地址向上增长）|  
+---------------------+
|     全局/静态区      |
+---------------------+
|      代码区         |
+---------------------+


#### 24.define/const/constexpr区别和应用场景
区别：
- define，宏定义，在编译前完成，无作用域全局有效
- const，用来定义常量，运行时确定，有类型和作用域
- constexpr，用来定义编译时计算的量，编译时确定，提高运行时性能，有类型和作用域

#### 25.lambda的应用


#### 26.virtual和override的区别和应用
override用来强调对虚函数的重写，使得代码的可读性更高，但其实可以不用。

#### 27.sizeof 和 strlen 的区别
strlen 是头文件 cstring 中的函数，而 sizeof 是 C++ 中的运算符
sizeof 返回类型的字节，strlen计算字符串的长度(且不包括'/0')

#### 28.*int const a和const *int a有什么区别
前者表示常量指针，即指针本身不能被修改，但指向的值可以被修改；
后者表示指向常量整数的指针。
(巧记方式:就近原则，const接近指针a，所以修饰指针，const接近int，修饰值本身。)

#### 29.scanf和cin有什么区别，为什么scanf比较快
cin类型安全，自动推导类型，无需手动指定格式符，又因为要实现上面这些功能，所以涉及到更多的抽象和封装，自然比较慢。scanf则直接与底层IO交互，要手动指定格式符。

此外影响比较大的是缓冲机制，cin使用标准库的流缓冲机制，默认与stdio同步，这会带来额外的开销。



#### 30.几种类型转换之间的区别
- static_cast <new_type> (expression),用于基本类型/指针/引用类型之间的转换，和C语言中做强制类转换基本等价
- dynamic_cast <new_type> (expression)，常用于将基类指针或引用转换为派生类指针或引用，dynamic_cast可以确保类型兼容性。dynamic_cast只有在基类存在虚函数(虚函数表)的情况下才有可能将基类指针转化为子类，否则返回异常。其依赖运行时信息(RTTI)来判断类型是否正确。
- const_cast <new_type> (expression),其中new_type必须是一个指针/引用，用来抹去const属性，比如修改const对象，或者用于const对象调用非const成员函数。
- reinterpret_cast <new_type> (expression)，用于在不同类型中进行低级别转换。仅仅是重新解释底层比特（也就是对指针所指针的那片比特位换个类型做解释），而不进行任何类型检查。
  
#### 31.内联函数知道吗？有什么好处？
内联函数在编译时会被直接展开，而不是通过函数调用的方式执行，这样就避免了栈帧的创建和销毁/参数传递/返回地址保存等操作。
适用于小型且频繁调用的函数。但inline只是对编译器的建议，编译器可能会忽略内联请求。

# OS
#### 1.消息队列有了解过吗？如何保证信息不丢失？如何保证一致性？
常见的有生产消费者队列，kafka，RabbitMQ。
经常用来将耗时操作异步化，解耦生产者和消费者，通过消息队列缓冲请求避免过载。

保证不丢失：
- 生产者端:生产者发送消息后，等待消息队列的ACK确认，或者发送持久化消息。RabbitMQ都支持这两个机制。
- 消息队列端：持久化存储，如kafka的日志文件持久化机制；高可用性，kafka的副本机制。
- 消费者端：消费者处理完消息后，手动发ACK确认，通过唯一ID实现幂等性处理。

保持一致性：
- 本地事务表
- kafka的事务机制
- 重试机制
- 回滚机制

#### 2.fork()的具体过程？
1.复制父进程
当调用 fork() 时，操作系统会复制父进程的地址空间（包括代码段、数据段、堆、栈等）到子进程。
子进程是父进程的完整副本，包括：
- 程序计数器（PC）：子进程从 fork() 的返回处开始执行。
- 打开的文件描述符：子进程继承父进程打开的文件描述符。
- 进程状态：包括寄存器、信号处理函数等。
2.写时复制（Copy-On-Write, COW）
现代操作系统使用写时复制技术优化 fork() 的性能。
在 fork() 调用时，子进程与父进程共享相同的物理内存页，而不是立即复制内存。
当父进程或子进程尝试修改内存页时，操作系统才会复制该内存页，确保父进程和子进程的内存独立性。
3.并发执行
fork() 调用后，父进程和子进程并发执行，执行顺序由操作系统调度器决定。

#### 3.fork()都会返回吗？
返回结果：
在父进程中，fork() 返回子进程的 PID。
在子进程中，fork() 返回 0。
如果 fork() 失败，返回 -1，并设置 errno 表示错误原因。

#### 4.linux命令行输入命令后的执行过程？
- shell解析用户输入的命令
- 查找可执行文件
- 调用fork()创建子进程
- 调用exec()系列函数执行命令，加载可执行文件
- 操作系统加载可执行文件到内存，解析可执行文件的格式，加载代码段数据段等
- 移交CPU控制权给可执行程序，执行完毕后将结果输出到标准输出(通常是终端)
- 它们在做这些时，shell调用wait()，等待子进程执行完毕
- 子进程退出后，回收进程描述符和内存和退出状态
  
#### 5.CAS操作了解吗？
原子操作，本质上是硬件支持的原子指令。
CAS 操作比较内存中的值与期望值，如果相等，则将内存中的值更新为新值；否则，不做任何操作。
这些步骤都是原子的，操作过程中不会被其它线程打断。

经常用于原子加减/无锁队列/线程安全的计数器。

优点：避免了锁的开销，性能更好，但编码更复杂。
缺点：ABA问题，即内存中的值从A变成B又变成A，CAS无法检测到这种变化；忙等待，CAS操作会多次尝试。

解决ABA：增加版本号

#### 6.int和uint和浮点数的二进制表示，每一位的含义？
int:	补码表示，最高位为符号位，其余位表示数值大小。
uint:	无符号二进制表示，所有位表示数值大小。
浮点数:	IEEE 754 标准表示，分为符号位、指数位和尾数位。

#### 7.进程和线程的区别，相关系统调用
进程是正在运行的一段程序，线程是进程当中的一条执行流程。
线程最主要的特点是并发运行、共享相同的资源(代码段、数据、文件)但是各自有独立的寄存器和栈。
比如有份代码

```cpp
main(){
  while(1){
     read();
     unZip();
     Play();
  }
}
```
单进程播放的话，只能顺序执行，这显然是不对的，音画都不连贯了，事实上可以同时做。如果简单地改成多进程，那进程间该如何通信呢？(此处不表，见下一道)

所以这就引入了线程。开3个线程read()、unZip()、Play()并放到线程池里，
就能并发执行了。
比较进程和线程：
- 线程是调度的基本单位，进程是资源拥有的基本单位
- 线程更快，因为能共享资源，不需要太多额外的信息，所以创建快、终止快、切换快(共享页表)

相关系统调用:
进程:fork(),exec(),exit(),wait(),getpid()
线程:pthread_create(),pthread_exit(),pthread_join(),pthread_mutex()

8.锁有哪些种类？互斥锁和自旋锁的区别与具体应用？能不加锁吗？
- 互斥锁（Mutex）就像房间钥匙，在任何时刻只有一个线程能够拿到这把钥匙，否则就被阻塞，直到锁被释放。这会带来线程上下文的切换。互斥锁常用于临界区较长的场景，如多线程同时读写一个文件/访问数据库/共享数据结构。

- 自旋锁（Spin Lock）自旋锁不会阻塞，而是不断检查锁是否被释放。线程不进入睡眠状态，一直占用 CPU 资源进行循环等待，所以称为 “自旋”。自然就避免了线程上下文切换的开销。自旋锁适用于锁的持有时间非常短，且线程竞争不激烈的场景，例如多个线程同时更新一个计数器(该操作通常非常快)或者内核编程中短时间同步

- 读写锁（Read-Write Lock） 允许多个读线程，写线程独占，适用于读多写少的场景。

- 条件变量（Condition Variable）条件变量通常与互斥锁配合使用，用于线程之间的通信和同步。它允许线程在满足特定条件时才进行相应的操作，否则就等待。线程可以在条件变量上等待，直到其他线程通过信号或广播的方式通知它条件已经满足，才会被唤醒继续执行。
  
- 能不加锁，但更为复杂。比如TBB和boost就提供了基于原子操作或者细粒度实现的容器。

#### 9.用户态内存分为哪些区域？
- 代码段，存储程序代码
- .data，存储已初始化的全局变量
- .bss，未初始化的全局变量
- 堆，分配动态内存
- 栈，存储局部变量和参数
- 共享库，存储动态链接库的代码和数据
- 内存映射区域，内存映射文件或匿名内存映射(mmap，比如从内存中读数据出来可以放在这里，可以开辟一块内存让进程共享)

#### 10.一段程序经历的阶段？
- 写代码
- 预处理(处理预处理指令，如include和define)
- 编译，gcc/clang将预处理后的源代码编译成汇编代码.s文件
- 汇编，将汇编代码翻译成机器代码，生成目标.o文件
- 链接，将目标文件与库文件链接，生成可执行文件，输出.exe文件
- 加载，将可执行文件加载到内存中，然后是运行和结束

#### 11.如何保证一个类是线程安全的？
- 使用mutex+条件变量保护数据，实现同步
- 使用原子操作
- 不需要改动的对象尽量用const修饰
- 无锁数据结构

#### 12.i++操作是线程安全的吗？
不是，这分为读取i，+1，写回，这三个操作之间随时可能被打断。
可以加互斥锁/自旋锁，用原子操作实现。

#### 13.生产消费者模型中锁是如何实现的？


#### 14.进程间通信方式？

这个问题很大，尽量简单讲。本来进程的用户地址空间是独立的，不能互相访问，但所有进程
都共享内核空间，所以进程之间通信必须通过内核。
- 管道(内核里面的一串缓存)。linux命令里有个"|"，意思是把前者的输出作为后者的输入，这就算单向通信。
通信的话需要有来有往，管道的设计是让一个进程同时连管道的读端和写端，这怎么通信？
A：再fork一个子进程(fork能复制上一级进程的文件描述符)，这样两个进程都接到了管道里，就可以读写同一个管道了。你也可以简单地理解为它们约好了一个神秘地点，要写东西和拿东西都来这里，所以也特别慢。

- 消息队列(保存在内核中的消息链表)。A 进程要给 B 进程发送消息，A 把数据放在对应的消息队列后就可以返回，B 进程需要的时候再去读就行。和管道不同，匿名管道随着进程创建完就销毁，消息队列会随着内核一直在。消息就像邮件，不及时、有附件限制、数据拷贝开销(因为是保存在内核中的，所以用户态内核态之间读写时，肯定有拷贝开销)。

- 共享内存。本来每个进程都有独立的虚拟空间(当然了，进程的虚拟内存会映射到不同的物理内存，才不会互相影响)，共享就是拿出一块虚拟地址映射到相同的物理内存。这样一个进程一写入，另一个马上能看，不用拷贝。具体的，可以用shmget创建共享内存。

- 信号量。既然共享，就会冲突。用信号量来实现任意时刻资源只能被一个进程访问的保护机制。信号量是一个int，有PV操作，在分类上也分为同步信号量和互斥信号量。P操作：把信号量-1，如果此时信号量<0，表示被占用；如果>=0，说明还有资源可用。V操作：把信号量+1，如果<=0，表明还有进程阻塞着，就把它叫起来；如果信号量>0，说明当前没进程阻塞了。使用时P操作在进入共享资源前，V操作在离开共享资源后，必须成对出现。接下来就互斥信号量和共享信号量举个例子：
**互斥信号量**(初始值为1)：①进程A执行P操作，执行后信号量为0，资源可用，A使用。②如果此时B想访问，也执行了P，信号量变成-1，
资源不能用，B被堵塞。③A用完后，V操作让信号量恢复为0，发现有进程阻塞，所以把B叫起来。④等B用完后，V操作又让信号量恢复到1。
**同步信号量**(初始值为0)：①B要用资源，执行完P发现信号量是-1，表示A还没产生数据，B就阻塞等待。②A产生完数据后，V操作让信号量变成0，唤起B。
所以同步信号量能保证进程A在进程B之前执行。

- socket通信。是的，不仅能在网络间通信，进程间也行。不过bind的不是IP地址和端口，而是绑定本地文件。


#### 15.进程间共享变量怎么做？
相当于进程间通信，参考上一题。

#### 16.为什么要用虚拟内存？为什么要分段和分页？段页式？
- 隔离进程
- 内存共享，方便通信
- 突破物理内存限制，通过将暂时不用的页面交换到磁盘上，2G的程序能在只有1G物理内存的系统上运行。不用将所有程序都一次性载入(单片机就是一次性烧入)也体现了很优雅的设计原则。
- 页表有读写权限、标记该页是否存在，提供更好的安全性，这是一个小小的buff

操作系统为每个进程分配独立的一套「虚拟地址」，人人都有，大家自己玩自己的地址就。
那具体怎么映射呢？
- 内存分段。
把程序分成若干个逻辑分段(比如代码一段、数据一段、栈一段)，每段分开放。
那就得知道每段对应到哪里，我们用段表来表示这个映射。段表由段基地址+段界限组成，一个段拿着自己的段号来表里找对应的基地址，加上偏移量就能找到物理地址。比如代码段里需要访问偏移量为500的虚拟地址，在段表里找到代码段的基地址为7000，加上偏移量500=7500，7500就是物理地址了。
分段的思想很清晰，但容易有内存碎片，会产生多个不连续的小物理内存，导致新的程序无法被装载。而这些小物理内存的空间如果安排得当，又是能连在一起放更大的进程的。这样内存空间无法连续利用的碎片叫外部内存碎片。解决手段是内存交换(Linux里的swap)，先把某程序占用的内存写到硬盘上，再读回来，不过读取回来时不装到原来位置，而是紧贴着某块被占用的内存后面。这样就能保证空间尽量连续。
看起来很美好，但硬盘读写太慢了，所以分段从机制上就不行，接下来我们考虑内存分页。

- 内存分页。分页裁剪得更细致，不是按逻辑分段，而是把虚拟空间、内存空间都切成一段段固定的“页”，linux下页的大小是4KB。
这样映射时，虚拟内存的一个页对应到物理内存的一个页，页之间都是贴着的，就不会有外部碎片。但可能数据用不满4KB，所以就出现了内部碎片。要关注的另一个问题是分页时怎么地址转换的？和分段有段表一样，分页也有页表。页表存着虚拟页对应物理页的基地址，基地址加上偏移量就是真实地址。
看起来很美好，但有大问题：页表会很大很大。考虑用多级页表解决：原本32位linux下每个进程拥有4G的虚拟地址，每个虚拟地址都需要映射，页大小位4KB，所以4G一共需要$4*1024*1024/4=1048576$这么多条。页表里每个项占用4Byte，所以一个页表要占$1048576*4(Byte)=4MB$。
考虑把这100多万项再分页，分成$1024(第一级)*1024(第二级)$，可以理解为原本是一维数组，现在压缩成了二维数组，但空间不变。
理论上二级表占用的空间是4KB(一级页表)+4MB(二级页表)好像还多了一级页表的开销，但事实上有“局部性原理”，其实我们用不了那么多空间。
所以如果一级页表的某个项没被用到，就不用创建对应二级页表，这部分就节省了很多空间。
热知识：64位的系统页表是4级的。

- 段页式管理。上述两种方法各有千秋，但不是对立的，实践中经常组合起来使用。先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页。这样，地址结构就由段号、段内页号和页内位移三部分组成。当然也要建立对应的段表、页表了。


#### 17.什么是内核态，什么是用户态
内核态是CPU的一种运行模式，操作系统内核就在此模式下运行。
具有最高权限，可以执行CPU所有指令，访问所有硬件资源和系统内存，往往用在系统调用/中断处理/设备驱动。
而用户态则只能执行非特权指令，只能访问分配给它的内存和资源，往往用在用户程序/库函数。

#### 18.异常和中断的区别？
异常与程序执行相关，是程序执行时发生的错误或特殊情况(如/0，访问非法内存，缺页)，同步发生，接下来要处理程序错误或者系统调用；中断与外部事件相关，是外部设备或定时器发出的信号(如硬件中断有io输入，定时器每隔一段时间触发中断)，异步发生，接下来要处理外部事件或者时间片轮转。

# 数据库
#### 1.什么时候该建立索引？什么时候不该建立索引？如何建立索引？
以下情况的列适合建立索引：
- 频繁查询，比如经常出现在where/order by/group by的字句中，索引可以加速查询
- 唯一性约束，索引的底层是B+树或者哈希表，可以快速查找
- 高选择性列，比如身份证号这类分布比较分散的，性别这种分布均匀的就没必要
- 外键连接，索引可以加速连接，避免全表扫描

不该建立索引：
- 频繁更新，索引会增加开销
- 底选择性，如状态/性别
- 数据小，全表扫描可能更快，索引维护起来还是很精细的

如何建立索引？索引大扫盲：
- 单列索引，在单个列上建立索引
- 多列索引，建立组合索引(这里涉及到最左前缀原则，待会讲)
- 唯一索引，确保列值的唯一性
- 主键索引，主键列会自动创建主键索引
- 全文索引，支持文本列的全文索引
- 覆盖索引，在查询只需要索引列的数据时，就可以建立覆盖索引(理解为该索引可以覆盖全部数据)，避免访问表数据


#### 2.查询索引一定更快吗？分析下列情况怎么使用索引会更快？
和上题本质一样。


#### 3.数据库内核是怎么决定是否使用索引查询以及该用哪个索引查询？如何sampling，怎么计算查询成本和生成执行计划？
首先，数据库里有个东西叫查询优化器。查询优化器通过成本模型和统计信息来评估不同查询计划的成本选最好的。
先细讲查询优化器，该组件会从以下几个因素出发判断是否用索引：
- 涉及到索引列
- 选择性
- 索引可以覆盖查询的所有列
- 匹配的数据量较小时用索引是更好的，否则极端情况下不如全表扫描

在考虑用哪个索引时，会优先选择能覆盖查询条件的索引或者高选择性的索引(比如查where name = 'alice'肯定优先考虑name索引列)

这里的sampling指的是采集表的行数/列的值分布/索引的选择性，通过随机采样，分层采样，系统表实现。

计算查询成本时要考虑到CPU成本，IO读取的成本，使用内存的开销。具体的，常见的成本估算有全表扫描的成本，读取索引页和访问数据页(num_index_pages * page_cost + num_data_pages * page_cost)的成本。

生成执行计划时要先将SQL解析成抽象语法树，然后从语法的角度进行逻辑优化，再从物理角度(选择合适的索引)进行优化，最后选最好的。

#### 4.如何让索引尽可能加速查询操作？
合理设计索引，定期重建索引，对大表进行分区减少索引的查询范围。

#### 5.回表是什么？如何减少回表操作？
数据库需要根据索引找到的主键或行指针，再到数据表中查找完整的数据行。回表操作会增加额外的 I/O 开销，尽量用覆盖索引，使用聚簇索引(比如InnnoDB主键索引是聚簇索引，直接在叶子节点存储行数据，避免回表操作)。

#### 6.如果数据库的io很大，有哪些可能性，该如何排查并定位原因，该如何优化？

#### 7.数据库还有哪些性能指标？

#### 8.为什么主键要用自增id？

#### 9.覆盖索引了解吗，具体怎么用？

#### 10.为什么用B+树不用B树？为什么B+树胖且矮？如果把内容存在内存上，用B树会不会快一点？

#### 11.B+树和B树的时间复杂度有什么区别，有的话差多少？

#### 12.(a1,a2,a3)查找(a2,a3)会找到a1吗(最左前缀原则)？

#### 13.怎么保证数据库和redis一致性？

#### 14.讲一下一条SQL语句查询的全过程

#### 15.讲一下隔离级别？

#### 16.如何解决脏读/不可重读/幻读？能不加锁吗？

#### 17.知道MVCC吗？

#### 18.redo log和undo log有什么区别？


# 计网
## TCP
#### 1.TCP三次握手过程/四次挥手过程？为什么是三次，为什么是四次？多一次少一次不行吗？
参考 https://csguide.cn/network/transport/20_tcp_handshake.html
三次握手：

是因为TCP是全双工的，本来要先发一个ack表示我收到了，再发一个syn表示我也想跟你建立连接，
这样有来有往了两次后才建立起连接，然后我们发现ack和syn可以合在一起，于是四次变成了三次。

再少一次的话，没法保证连接的发起方有收到ack+syn报文，再多一次则没必要，已经能够基本保证双方在线，再多就
浪费网络资源。

四次挥手：
挥手则比较特别，涉及到close_wait和time_wait这两个要重点考虑的状态。
假设断开连接的那方为A，被断开的为B，A发fin跟B说我要跟你断了，B回了一个ack表示我知道了。
但并不会立马断开连接，而是变为 CLOSE_WAIT状态。B会通知应用层对方想要关闭连接，CLOSE_WAIT 状态将一直持续到应用层调用 close()，这期间要处理：完成剩余数据的发送/清理资源。如果应用程序有 bug 没有正确调用 close()，连接会一直停留在 CLOSE_WAIT 状态，造成资源泄漏。

等到B确定自己手上没事了，可以断开后，再发送fin给A，A收到了后发ack给B，表示自己知道你的答复了。至此，已经完成了4次，没法像刚才握手那样合并成3次是因为挥手时B的ack和syn不能合并在一起发送，所以就是本来的四次。

A端在发出ack后，进入time_wait状态，通常会持续2MSL(最大报文生存时间)，这是为了使得该连接中所有的报文都从互联网中消失，不会跟后面混淆。同时，可能这次发出的ack消失了，B端等了很久一直没收到答复，会再发一个fin，这时A就能正确处理。如果一发出去就不管，可能没法保证我的fin正确被对方收到。因此，time_wait状态既是为了给本次连接收尾，也是为了确保fin报文能顺利到达。


#### 2.syn序列号是怎么确定并发送的？
syn序列号最开始的值也称为ISN，是一个32位的随机数，随机是为了增加不可预测性防止攻击。
在接下来的连接中，syn序列号会随着数据字节的发送递增，一个字节+1。

#### 3.为什么ack回应的序列号是x+1？
1.能表达：接收方真的收到了
2.能回复：我现在需要你的第x+1号数据，发送方才能知道接收方收到了哪些，现在需要什么。

#### 4.简述DDos攻击，该如何防治？
一般是syn flood或者udp flood，在短时间内向某个服务器发送大量的请求，使得服务器处理不过来，耗尽目标资源。
防治的话考虑：
- 专业的DDos防护服务，如Cloudflare(每次登codeforces都要被这玩意卡一下，原来是在检测我的流量正不正常)。cloudflare
有一个全球分布的网络，能吸收和分散攻击流量，此外还能通过历史数据/行为分析来检测请求是否为异常并拦截。
- CDN加速，CDN加速的原理同样也是把静态页面分布在全球，请求会到离自己最近的节点加载静态资源。
- 负载均衡，将流量分散到多台服务器，避免单点过载

#### 5.RST有什么具体作用？
RST（Reset） 是 TCP 协议中的一个标志位，用于立即终止 TCP 连接。它的作用是在某些异常情况下强制关闭连接，而不是通过正常的四次挥手过程。
- 检测到异常情况（如非法连接、协议错误等），直接用RST终止
- 拒绝请求/认为连接已经失效或不再需要时，用RST

#### 6.MTU是什么？
单个数据包能携带的最大数据量，一般为1500字节左右，以太网/wifi/vpn的MTU会不同，但不会偏1500太多。

#### 7.TIME_WAIT有什么用？
见1.TCP三次握手过程/四次挥手过程？

#### 8.比较TCP和UDP
TCP面向连接，可靠，保证顺序，有流量控制和拥塞控制机制，因此效率也较慢，头部开销较大，适用于网页浏览/文件传输/电子邮件。
UDP无连接，不可靠，不保证顺序，没有流量和拥塞控制机制，因此效率也较快，头部开销较小，适用于视频流媒体/在线游戏/语音通话。

#### 9.TCP为什么会粘包(Nagle算法)和解决方法？
TCP粘包是因为Nagle算法。Nagle算法为了避免多次发送很小的数据包导致浪费，会把多个数据包合并成一个大数据包，导致接收方无法分辨
数据的边界。
一个流氓的解决办法是直接把Nagle算法ban了，因为它在linux里是默认开启的，可以关掉。但实践中其实是在每个数据包头部添加长度字段，接收方先读取长度字段，再根据长度读取数据。

#### 10.怎么设计一个可靠且报文有序的UDP协议？

#### 11.讲一下TCP的滑动窗口机制，滑动窗口大小如何确定？

#### 12.全连接队列与半连接队列了解吗？
收到了syn报文，发出了ack+syn，但还没等到ack确认时，这些连接先放入半连接队列。
等收到了ack确认后，再把连接从半连接队列移出，放进全连接队列。

#### 13.如果出现很多次close_wait怎么解决？close_wait和time_wait分别在哪个阶段？

#### 14.TCP建立连接个数可以超过端口号个数吗？如何确定唯一的TCP连接？
可以，因为TCP连接是由四元组确定的(源IP，源端口，目标IP，目标端口)，显然四元组的组合数目可以超过端口号个数。

#### 15.流量控制与拥塞控制区别？

#### 16.输入一个URL，计算机会发生什么？

## HTTP

#### 1.http请求由哪部分构成？
- 请求行。请求方法如get/put/delete + 请求目标如/index.html + http版本如http1.1），如
`GET /index.html HTTP/1.1`
- 请求头。包含多个键值对。如Host/User-Agent/Accept/Authorization/Cookie
- 空行\r\n。用来分割请求头和请求体
- 请求体。包含真正要发送的数据
  
#### 2.常见的请求头有什么？
- Host：表示请求的目标主机和端口，如Host: www.example.com。
- User-Agent：表示客户端的类型和版本，如User-Agent: Mozilla/5.0。
- Accept：表示客户端能够接收的响应内容类型，如Accept: text/html。
- Content-Type：表示请求体的数据类型（通常用于 POST 或 PUT 请求），如：Content-Type: application/json。
- Content-Length：表示请求体的长度（以字节为单位），如：Content-Length: 123。
- Authorization：表示客户端的身份验证信息，如：Authorization: Bearer <token>。
- Cookie：表示客户端发送的 Cookie 信息，如：Cookie: name=value。

#### 3.get和post的区别？
get通常用来获取资源，post用来提交表单。

#### 4.HTTP用TCP还是UDP？
通常是TCP，但http3引入了QUIC协议，基于UDP实现的，其在UDP基础上实现了快速连接建立和加密，减少了握手延迟。

#### 5.常见状态码
200 是成功
300 是重定向
400 是自己有问题
500 是服务器有问题

#### 6.http是长连接还是短连接，如何用UDP实现
这个得看具体情况讨论。
- http1.0默认是短连接，每次请求完成后，都会关闭TCP连接。
- 从http1.1开始，默认开启keep-alive，也就是长连接机制，可以在一次TCP连接中发送多个http请求和响应。

用UDP实现的话：
- 可靠性方面:由于 UDP 不可靠，需要在应用层实现可靠性机制，如：确认机制（ACK）：接收方收到数据后发送确认消息；重传机制：如果发送方未收到确认消息，重新发送数据；序列号：为每个数据包添加序列号，确保数据顺序。
- 长连接模拟:
在 UDP 中模拟长连接，可以通过会话标识：为每个会话分配唯一标识符，客户端和服务器根据标识符维护会话状态；心跳机制：定期发送心跳包，检测连接是否存活。

#### 7.使用HTTP从服务器获取数据，数据有长有段，如何接受？
http的请求头里有Content-Length，表示数据长度，还有Transfer-Encoding，表示分块发送。
读取这两个的值然后判断是直接根据长度去读，还是分块接受就好了。

#### 8.HTTP为什么要用非对称加密和对称加密？为什么不只用一个？
见2.HTTPS怎么保证安全通信？同时用是为了保证安全性和速度。

#### 9.知道http有几个版本吗，2和3有什么区别

## 网络安全
#### 1.对称加密与非对称加密过程？
对称加密是只有一个密钥，拿同一个密钥加密解密。
非对称加密是有一对密钥，拿公钥加密，拿私钥解密，且无法从公钥推导出私钥。

#### 2.HTTPS怎么保证安全通信？
基于TLS握手/SSL加密/数字证书。
具体的，保密层是：
- 客户端生成一个随机的对称密钥（会话密钥），用于后续的加密通信。
- 客户端使用服务器的公钥加密对称密钥，并将加密后的密钥发送给服务器。
- 服务器使用自己的私钥解密客户端发送的对称密钥。
- 客户端和服务器使用对称密钥加密和解密后续的通信数据，对称加密速度快，适合加密大量数据。


## 其它协议
#### 1.知道ICMP吗？

#### 2.7层，5层，4层协议？每层的常见协议？

#### 3.cookie和session的区别？

## 服务器底层
#### 1.epoll和select和poll有什么区别

#### 2.什么时候用水平触发，什么时候用边缘触发？

#### 3.一个TCP句柄什么情况下会有可读事件？
- 接收缓冲区中有数据
- 对方关闭数据，会触发可读事件，如果读到的返回值为0，说明对方关闭连接
- 监听到新的连接也会触发可读事件
- 收到tcp错误时，也会，如果读到的返回值为01表示发生了错误。

#### 4.TCP的connect发送了什么报文?
三次握手。

#### 5.UDP的句柄能执行connect吗？
行为不同。UDP的connect() 并不是用于建立连接（因为 UDP 是无连接的协议），而是用于绑定默认的远程地址和端口

# 中间件
#### 1.zookeeper了解吗？

#### 2.MySQL/Redis/Kafka等在高性能，高可用，高并发方面有什么设计？
Redis有哨兵集群和切片集群。

#### 3.Redis的缓存三姐妹(缓存穿透/缓存击穿/缓存雪崩)？手动更新完数据库，导致缓存失效了，大量的数据全打在数据库上怎么处理？

#### 4.etcd的原理？

#### 5.redis和mysql区别(?)

#### 6.MySQL底层使用的数据结构？

#### 7.Redis是单线程单进程的吗?在服务器上也是单线程单进程的吗？



# 设计模式
#### 1.单例模式怎么实现？

#### 2.怎么实现一个线程一个单例？

# Linux
#### 1.查看进程系列命令:
- ps-aux第二列是什么
- ps-aux和ps -elf的区别

# 脑筋急转弯
#### 1.10个红球和10个白球放俩盒子里，怎么分配使得从这俩盒子中摸一个球是红球的概率最高？

#### 2.25匹马，5个跑道，最少需要多少次比赛选出最快top3？

#### 3. 有1000瓶水，只有1瓶有毒，有无限老鼠，老鼠可以喝任意瓶。毒药会在24小时后发作，请在一天内
用最少老鼠找出哪瓶有毒？


# 场景题
#### 1.1s内有大量用户访问，怎么缓解服务器压力？

#### 2.内存泄漏怎么解决？

#### 3.如果有一个错误的提交怎么把它退回到正确的，包括本地和远端都要回退？

#### 4.gdb在程序运行时如何调试？

#### 5.设计一个应用层使用TCP应该注意哪些？


# 高频lc
- 链表反转
- 链表归并
- LRU缓存
- k个链表归并
- k个一组反转链表且空间O(1)时间O(n)
- 重排链表
- 括号匹配
- 寻找avl第k大的数
- 三数之和
- 未出现的最小正整数且空间O(1)时间O(n)
- 判断字符串s是否为字符串t首尾连接构成的环的一部分
- 找出数组中第k大的数
- 主站1081
